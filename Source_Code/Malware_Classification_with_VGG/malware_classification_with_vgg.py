# -*- coding: utf-8 -*-
"""Malware_Classification_with_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gwYnYEPlLsUb8Wqdq5TpgaHtJxUYa2Nj
"""

from google.colab import drive

drive.mount('/content/drive')

# runtime 시작시에는 주소가 /content 에서 시작함

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import os
import tensorflow as tf
from tensorflow.keras import Model

from tensorflow.keras.layers import Input, Conv2D, Add, BatchNormalization, GaussianNoise, Activation
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img

from PIL import Image
# %matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import keras
import pandas as pd
import shutil

base_dir = 'drive/My Drive/Colab Notebooks/Malware Classification Using CNN'
mal_img_dir = base_dir + '/malware_500_img'
ben_img_dir = base_dir + '/benign_500_img'


# mal_test_img_dir = base_dir + '/malware_500_resized_test_img'
# ben_test_img_dir = base_dir + '/benign_500_resized_test_img'

# len(os.listdir(mal_img_dir))

def is_png(input_file):
    ext = os.path.splitext(os.path.basename(input_file))[1]
    print('ext = ', ext)
    result = False
    if '.png' == ext:
        result = True
    return result


def preprocess_img(img_path, target_size=32):
    from keras.preprocessing import image

    img = image.load_img(img_path, target_size=(target_size, target_size))
    img_tensor = image.img_to_array(img)

    img_tensor = np.expand_dims(img_tensor, axis=0)

    img_tensor /= 255.

    return img_tensor


def gather_img(input_dir):
    img_train_set = []
    img_test_set = []

    if not os.path.isdir(input_dir):
        print(input_dir, 'Input directory not found. Exiting.')
        exit(0)

    count = 0

    png_list = os.listdir(input_dir)

    img_num = len(png_list)

    img_num_cnt = 0
    for filename in png_list:
        if is_png(filename):

            print("filename : ", filename)

            img_path = os.path.join(input_dir, filename)

            img_tensor = preprocess_img(img_path, target_size=32)

            tmp_train_len = len(img_train_set)
            tmp_test_len = len(img_test_set)

            if img_num_cnt < (int(0.8 * img_num)):
                img_train_set.append(img_tensor)
                if len(img_train_set) - tmp_train_len == 1:
                    print("append done! cnt : ")

            else:
                img_test_set.append(img_tensor)
                if len(img_test_set) - tmp_test_len == 1:
                    print("append done! cnt : ")

            img_num_cnt += 1

    return img_train_set, img_test_set


mal_img_set, mal_test_img_set = gather_img(mal_img_dir)
print(len(mal_img_set))
print(len(mal_test_img_set))

ben_img_set, ben_test_img_set = gather_img(ben_img_dir)
print(len(ben_img_set))
print(len(ben_test_img_set))

# dataset


# training
mal_img_set = np.array(mal_img_set)
ben_img_set = np.array(ben_img_set)

mal_num = len(mal_img_set)
ben_num = len(ben_img_set)

x_train = np.concatenate((mal_img_set, ben_img_set), axis=0)

y_train = [0 for _ in range(mal_num)]
for i in range(ben_num):
    y_train.append(1)

# test
mal_test_img_set = np.array(mal_test_img_set)
ben_test_img_set = np.array(ben_test_img_set)

mal_test_num = len(mal_test_img_set)
ben_test_num = len(ben_test_img_set)

x_test = np.concatenate((mal_test_img_set, ben_test_img_set), axis=0)

y_test = [0 for _ in range(mal_test_num)]
for i in range(ben_test_num):
    y_test.append(1)

x_train = x_train.squeeze()
print(x_train.shape)
y_train = np.array(y_train)

print(y_train.shape)
x_test = x_test.squeeze()
print(x_test.shape)
y_test = np.array(y_test)
print(y_test.shape)

"""
# cnn_model 3 (skip connection + bn)
input_shape = Input(shape=(32, 32, 3))

# for skip connection
x_shortcut = input_shape

x3 = Conv2D(filters=64, kernel_size=3, padding='same')(input_shape)
bn = BatchNormalization()(x3)
ac = Activation('relu')(bn)

x3 = Conv2D(filters=64, kernel_size=3, padding='same')(ac)
bn = BatchNormalization()(x3)
ac = Activation('relu')(bn)

x3 = Conv2D(filters=64, kernel_size=3, padding='same')(ac)
bn = BatchNormalization()(x3)
ac = Activation('relu')(bn)

x3 = Conv2D(filters=64, kernel_size=3, padding='same')(ac)
bn = BatchNormalization()(x3)
ac = Activation('relu')(bn)

x3 = Conv2D(filters=64, kernel_size=3, padding='same')(ac)
bn = BatchNormalization()(x3)
ac = Activation('relu')(bn)

x3 = Conv2D(filters=64, kernel_size=3, padding='same')(ac)
bn = BatchNormalization()(x3)
ac = Activation('relu')(bn)

x3 = Conv2D(filters=64, kernel_size=3, padding='same')(ac)
bn = BatchNormalization()(x3)
ac = Activation('relu')(bn)

x3 = Conv2D(filters=64, kernel_size=3, padding='same')(ac)
bn = BatchNormalization()(x3)
ac = Activation('relu')(bn)

x3 = Conv2D(filters=64, kernel_size=3, padding='same')(ac)
bn = BatchNormalization()(x3)
ac = Activation('relu')(bn)

x3 = Conv2D(filters=64, kernel_size=3, padding='same')(ac)
bn = BatchNormalization()(x3)
ac = Activation('relu')(bn)

x3 = Conv2D(filters=3, kernel_size=3, padding='same')(ac)

# skip connection : add input
x3 = Add()([x3, x_shortcut])

cnn_model_3 = Model(input_shape, x3)

cnn_model_3.compile(Adam(),
                    loss='mse',
                    metrics=['accuracy'])

"""

from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.applications.vgg16 import VGG16


from keras.applications.vgg19 import preprocess_input
from keras.applications.vgg19 import decode_predictions
from keras.applications.vgg19 import VGG19


from keras.models import Sequential
from keras.layers import Dense, Activation ,Flatten, Conv2D

vgg = Sequential()
vgg.add(Conv2D(3, (3, 3), padding = 'same', input_shape = x_train.shape[1:]))
vgg.add(Activation('relu'))

_vgg = VGG16(weights = 'imagenet', include_top = False)

counter = 0
for layer in _vgg.layers:
    layer.trainable = False
    counter += 1

print("VGG's ", counter, " layers are not added to the layer")

vgg.add(_vgg)
print("vgg model implementation done!!!")

vgg.add(Flatten())

vgg.add(Dense(32,activation='relu'))
vgg.add(Dense(1,activation="relu"))
#vgg.add(Dense(1, activation = "softmax"))

vgg.summary()
vgg.compile(optimizer='adam',loss='mse',metrics=['accuracy'])

batch_size = 32
epochs  = 200
model_info = vgg.fit(x_train, y_train,  epochs=epochs, batch_size=batch_size, verbose =1)

#hist = cnn_model_3.fit(x_train, y_train, batch_size=32, epochs=200, verbose=1)

test_model = vgg.evaluate(x_test, y_test, verbose =1)
print("test loss : ", test_model[0])
print("test accuracy : ", test_model[1])

#test_model = cnn_model_3.evaluate(x_test, y_test, verbose=1)
#print("test loss : ", test_model[0])
#print("test accuracy : ", test_model[1])
